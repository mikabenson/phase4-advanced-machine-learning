{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aeEkkaYn7EmV"
      },
      "source": [
        "# **Natural Language Processing**\n",
        "\n",
        "Natural Language Processing (NLP) is a subfield of Artificial Intelligence (AI) and Computational Linguistics that focuses on enabling machines to understand, interpret, and generate human language. It seeks to bridge the gap between human communication (natural language) and machine understanding, allowing computers to interact with us in a more intuitive way. NLP combines computer science, linguistics, and statistical models to process and analyze large amounts of natural language data. It has applications in various domains, such as:\n",
        "\n",
        "- **Search engines** (e.g., Google Search)\n",
        "- **Voice assistants** (e.g., Siri, Alexa)\n",
        "- **Text analysis** (e.g., sentiment analysis, topic modeling)\n",
        "- **Machine translation** (e.g., Google Translate)\n",
        "- **Chatbots and customer service automation**\n",
        "- **Social media monitoring** (e.g., detecting trends, sentiments)\n",
        "\n",
        "The ability to process and understand human language opens the door for computers to assist in tasks traditionally requiring human cognition, such as summarizing documents, understanding context, extracting key information, and generating coherent responses.\n",
        "\n",
        "### **Key Components of NLP**\n",
        "NLP typically involves a series of tasks to understand and process natural language, including:\n",
        "\n",
        "1. **Text Preprocessing:** This involves cleaning and preparing text for further analysis. Steps include tokenization, removing stopwords, stemming, and lemmatization.\n",
        "2. **Part-of-Speech Tagging (POS):** Identifying the grammatical components of a sentence, such as nouns, verbs, adjectives, etc.\n",
        "3. **Named Entity Recognition (NER):** Detecting entities (e.g., person names, locations, dates) within text.\n",
        "4. **Dependency Parsing:** Analyzing the syntactic structure of a sentence to understand how words are related to each other.\n",
        "5. **Sentiment Analysis:** Determining the sentiment (positive, negative, or neutral) expressed in a piece of text.\n",
        "6. **Machine Translation:** Converting text from one language to another using AI models.\n",
        "7. **Text Generation:** Generating new text based on learned patterns, as in chatbot or language models like GPT-3.\n",
        "\n",
        "NLP plays a crucial role in tasks such as search engine optimization (SEO), social media monitoring, language translation, and content personalization. As AI continues to evolve, the significance of NLP will only grow, offering even more sophisticated ways for machines to understand and interact with human language.\n",
        "\n",
        "### **Challenges in NLP**\n",
        "Despite significant advancements, NLP faces several challenges due to the complexity and ambiguity of human language. Some of these challenges include:\n",
        "\n",
        "- **Ambiguity:** Words can have multiple meanings depending on context (e.g., \"bat\" could refer to an animal or a sports equipment).\n",
        "- **Contextual Understanding:** Understanding nuances like irony, sarcasm, and cultural references.\n",
        "- **Language Variability:** Different dialects, slang, and informal language pose a challenge in accurate language processing.\n",
        "- **Data Quality:** Large amounts of labeled data are required to train NLP models, and this data can often be noisy or biased.\n",
        "\n",
        "To address these challenges, advanced techniques like deep learning, transfer learning, and transformer models (e.g., BERT, GPT) have significantly improved the capabilities of NLP systems.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rb2RjgZv7EmX"
      },
      "source": [
        "## **Natural Language Toolkit (NLTK)**\n",
        "\n",
        "### **Overview of NLTK**\n",
        "NLTK (Natural Language Toolkit) is one of the most popular Python libraries for working with human language data. It provides an extensive suite of tools and resources for:\n",
        "\n",
        "- **Text preprocessing**: Tokenization, stemming, lemmatization, and cleaning text.\n",
        "- **Linguistic analysis**: Part-of-speech (POS) tagging, syntactic parsing, and dependency analysis.\n",
        "- **Text classification**: Feature extraction and classification for tasks like sentiment analysis.\n",
        "- **Corpora access**: A wide variety of prebuilt datasets and lexicons, such as the Gutenberg Corpus and WordNet.\n",
        "\n",
        "#### **Why NLTK?**\n",
        "1. Comprehensive and versatile for NLP tasks, ranging from beginner to advanced use cases.\n",
        "2. Provides access to standard corpora and pre-trained models.\n",
        "3. Well-documented and easy to use for prototyping and educational purposes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bnJjV8pk7EmX"
      },
      "source": [
        "\n",
        "\n",
        "### **Key Concepts and Examples**\n",
        "\n",
        "#### **1. Corpus**\n",
        "A **corpus** is a collection of texts, often used as a dataset for training or testing NLP models. NLTK provides built-in corpora like `gutenberg` (classic books) and `brown` (annotated texts). You can also load your own datasets.\n",
        "\n",
        "**Example: Loading and Exploring a Corpus**\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "DExrozSZ7EmX",
        "outputId": "5c510c9f-4ac8-4f7a-b7c5-30d48e815f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "# List all files in the Gutenberg Corpus\n",
        "print(gutenberg.fileids())\n",
        "\n",
        "# Load text from a specific file (e.g., Jane Austen's \"Emma\")\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "print(text[:500])  # Print the first 500 characters\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Poems by William Blake 1789]\n",
            "\n",
            " \n",
            "SONGS OF INNOCENCE AND OF EXPERIENCE\n",
            "and THE BOOK of THEL\n",
            "\n",
            "\n",
            " SONGS OF INNOCENCE\n",
            " \n",
            " \n",
            " INTRODUCTION\n",
            " \n",
            " Piping down the valleys wild,\n",
            "   Piping songs of pleasant glee,\n",
            " On a cloud I saw a child,\n",
            "   And he laughing said to me:\n",
            " \n",
            " \"Pipe a song about a Lamb!\"\n",
            "   So I piped with merry cheer.\n",
            " \"Piper, pipe that song again;\"\n",
            "   So I piped: he wept to hear.\n",
            " \n",
            " \"Drop thy pipe, thy happy pipe;\n",
            "   Sing thy songs of happy cheer:!\"\n",
            " So I sang the same again,\n",
            "   While he wept with joy to hear.\n",
            " \n",
            " \"Piper, sit thee down and write\n",
            "   In a book, that all may read.\"\n",
            " So he vanish'd\n"
          ]
        }
      ],
      "source": [
        "text_poems = gutenberg.raw('blake-poems.txt')\n",
        "print(text_poems[:600])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[The King James Bible]\n",
            "\n",
            "The Old Testament of the King James Bible\n",
            "\n",
            "The First Book of Moses:  Called Genesis\n",
            "\n",
            "\n",
            "1:1 In the beginning God created the heaven and the earth.\n",
            "\n",
            "1:2 And the earth was without form, and void; and darkness was upon\n",
            "the face of the deep. And the Spirit of God moved upon the face of the\n",
            "waters.\n",
            "\n",
            "1:3 And God said, Let there be light: and there was light.\n",
            "\n",
            "1:4 And God saw the light, that it was good: and God divided the light\n",
            "from the darkness.\n",
            "\n",
            "1:5 And God called the light Da\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text_bible = gutenberg.raw('bible-kjv.txt')\n",
        "print(text_bible[:500])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXcXMdmh7EmZ"
      },
      "source": [
        "In this example:\n",
        "\n",
        "The gutenberg corpus includes texts from classic literature like Shakespeare and Jane Austen. By loading and inspecting a file (e.g., Emma), you can perform various analyses and operations on it, such as tokenization, sentiment analysis, or frequency analysis."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8zUlp047EmZ"
      },
      "source": [
        "## 2. Tokenization\n",
        "\n",
        "Tokenization is the process of splitting a large body of text into smaller units called tokens. These tokens can be words, sentences, or even subword components. Tokenization is typically one of the first steps in any NLP pipeline because it breaks down the text into manageable pieces.\n",
        "\n",
        "### Types of Tokenization:\n",
        "\n",
        "- **Word Tokenization**: Splitting text into individual words. This is useful when you want to analyze the frequency of words, part-of-speech tagging, or perform other word-level operations.\n",
        "- **Sentence Tokenization**: Splitting text into sentences. This is useful for sentence-level analysis such as sentiment analysis or parsing.\n",
        "\n",
        "Tokenization is essential because it allows subsequent NLP processes to work with smaller, more meaningful components of text.\n",
        "\n",
        "### Example: Word and Sentence Tokenization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# text = \"Natural Language Processing is exciting. It opens up many possibilities!\"\n",
        "\n",
        "# text_list = text.split()\n",
        "# text_list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokenization\n",
            "is\n",
            "essential\n",
            "because\n",
            "it\n",
            "allows\n",
            "subsequent\n",
            "NLP\n",
            "processes\n",
            "to\n",
            "work\n",
            "with\n",
            "smaller,\n",
            "more\n",
            "meaningful\n",
            "components\n",
            "of\n",
            "text.\n"
          ]
        }
      ],
      "source": [
        "text = \"Tokenization is essential because it allows subsequent NLP processes to work with smaller, more meaningful components of text.\"\n",
        "text_list = text.split()\n",
        "\n",
        "my_text = text_list\n",
        "\n",
        "for element in my_text:\n",
        "    print(element)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "# my_list = ['Natural', 'Language', 'Processing', 'is', 'exciting.', 'It', 'opens', 'up', 'many', 'possibilities!']\n",
        "\n",
        "# for element in my_list:\n",
        "#     print(element)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens (nltk): ['Tokenization', 'is', 'essential', 'because', 'it', 'allows', 'subsequent', 'NLP', 'processes', 'to', 'work', 'with', 'smaller', ',', 'more', 'meaningful', 'components', 'of', 'text', '.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "def nltk_tokenization_pipeline(text):\n",
        "    tokens = word_tokenize(text)\n",
        "    return tokens\n",
        "\n",
        "text = \"Tokenization is essential because it allows subsequent NLP processes to work with smaller, more meaningful components of text.\"\n",
        "tokens = nltk_tokenization_pipeline(text)\n",
        "print(\"Tokens (nltk):\", tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "LsUjNsLi7EmZ",
        "outputId": "4c12fcfe-2adf-47d4-f991-deceb02948e4"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers\\punkt.zip.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Natural Language Processing is exciting.', 'It opens up many possibilities!']\n",
            "['Natural', 'Language', 'Processing', 'is', 'exciting', '.', 'It', 'opens', 'up', 'many', 'possibilities', '!']\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize\n",
        "\n",
        "text = \"Natural Language Processing is exciting. It opens up many possibilities!\"\n",
        "# Sentence tokenization\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)\n",
        "# Output: ['Natural Language Processing is exciting.', 'It opens up many possibilities!']\n",
        "\n",
        "# Word tokenization\n",
        "words = word_tokenize(text)\n",
        "print(words)\n",
        "# Output: ['Natural', 'Language', 'Processing', 'is', 'exciting', '.', 'It', 'opens', 'up', 'many', 'possibilities', '!']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0eKVfH-l7EmZ"
      },
      "source": [
        "In this example:\n",
        "\n",
        "Sentence Tokenization splits the text into sentences using punctuation marks such as periods (.), exclamation points (!), or question marks (?).\n",
        "Word Tokenization splits the text further into individual words. Notice how punctuation marks like periods and exclamation points are treated as separate tokens."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rnFzSjb97EmZ"
      },
      "source": [
        "## 3. Stemming and Lemmatization\n",
        "\n",
        "Both stemming and lemmatization are techniques used to reduce words to their base or root form, allowing models to treat different variations of a word (e.g., \"running\" vs. \"run\") as the same entity.\n",
        "\n",
        "### Stemming:\n",
        "Stemming involves removing prefixes or suffixes from words, often resulting in roots that may not always be valid words. Stemming is a fast, rule-based method and is commonly used in information retrieval.\n",
        "\n",
        "- **Example**: \"running\" → \"run\", \"better\" → \"better\" (no change)\n",
        "\n",
        "### Lemmatization:\n",
        "Lemmatization is a more sophisticated technique that reduces words to their lemma, which is the dictionary form of the word. Unlike stemming, lemmatization ensures that the resulting words are valid dictionary entries. Lemmatization takes into account the word's context (part of speech) to produce the correct lemma.\n",
        "\n",
        "- **Example**: \"running\" → \"run\" (verb), \"better\" → \"good\" (adjective)\n",
        "\n",
        "### Example: Stemming vs. Lemmatization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'Tokenization is essential because it allows subsequent NLP processes to work with smaller, more meaningful components of text.'"
            ]
          },
          "execution_count": 17,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'tokenization is essential because it allows subsequent nlp processes to work with smaller, more meaningful components of text.'"
            ]
          },
          "execution_count": 16,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "text\n",
        "\n",
        "# perform stemming on the text..\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "# initialize stemmer and lemmatizer algorithms\n",
        "stemmer = PorterStemmer()\n",
        "stemmer.stem(text)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "1JiCGRz47Ema",
        "outputId": "cb6f639e-c057-4e68-b70c-95500c89a776"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Stemmed words: ['better', 'run', 'studi', 'fli', 'happier', 'illeg', 'sentiment']\n",
            "Lemmatized words: ['better', 'run', 'study', 'fly', 'happier', 'journey', 'write', 'sleep', 'sentimental']\n"
          ]
        }
      ],
      "source": [
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize stemmer and lemmatizer the algorithms\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Stemming example\n",
        "words_stemmed = [stemmer.stem(word) for word in [\"better\", \"running\", \"studies\", \"flies\", \"happier\", \"illegal\", \"sentimental\"]]\n",
        "print(\"Stemmed words:\", words_stemmed)\n",
        "# Output: ['better', 'run', 'studi', 'fli', 'happi']\n",
        "\n",
        "# Lemmatization example\n",
        "words_lemmatized = [lemmatizer.lemmatize(word, pos='v') for word in [\"better\", \"running\", \"studies\", \"flies\", \"happier\", \"journey\", \"wrote\", 'slept', \"sentimental\"]]\n",
        "print(\"Lemmatized words:\", words_lemmatized)\n",
        "# Output: ['better', 'run', 'study', 'fly', 'happier']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUWfKQJ_7Ema"
      },
      "source": [
        "**In this example:**\n",
        "\n",
        "- Stemming reduces words like *studies* to *studi* and *happiness* to *happi*, which may not be correct forms of the words.\n",
        "\n",
        "- Lemmatization ensures that words are reduced to valid base forms, like *study* instead of *studi*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hHVGHUtm7Ema"
      },
      "source": [
        "## 4. Part-of-Speech (POS) Tagging\n",
        "\n",
        "Part-of-speech tagging is the process of identifying the grammatical category (e.g., noun, verb, adjective) of each word in a sentence. POS tagging helps identify the role that each word plays in a sentence, enabling more advanced tasks like syntactic parsing or named entity recognition.\n",
        "\n",
        "### Why is POS tagging useful?\n",
        "\n",
        "- It helps in extracting useful information from text, such as identifying nouns, verbs, and adjectives.\n",
        "- It is crucial for syntactic parsing, where the structure of a sentence is analyzed.\n",
        "- It can also be used in tasks like named entity recognition (NER) to identify people, locations, and other entities in text.\n",
        "\n",
        "### Example: POS Tagging\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BuL4tXvQ7Ema",
        "outputId": "22b0fdbd-bba5-435c-dde7-1304a4322b0d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('exciting', 'VBG'), ('!', '.')]\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package punkt_tab to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package punkt_tab is already up-to-date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
            "[nltk_data]       date!\n",
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
            "[nltk_data]       date!\n"
          ]
        }
      ],
      "source": [
        "from nltk import pos_tag\n",
        "from nltk.tokenize import word_tokenize\n",
        "nltk.download('punkt_tab')\n",
        "nltk.download('averaged_perceptron_tagger_eng')\n",
        "\n",
        "import nltk\n",
        "nltk.download('averaged_perceptron_tagger')\n",
        "\n",
        "# Sample text\n",
        "text = \"Natural Language Processing is exciting!\"\n",
        "\n",
        "# Tokenize and POS tagging\n",
        "tokens = word_tokenize(text)\n",
        "tagged = pos_tag(tokens)\n",
        "print(tagged)\n",
        "# Output: [('Natural', 'JJ'), ('Language', 'NNP'), ('Processing', 'NNP'), ('is', 'VBZ'), ('exciting', 'JJ'), ('!', '.')]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from nltk import pos_tag\n",
        "# from nltk.chunk import ne_chunk\n",
        "# from nltk.tokenize import word_tokenize\n",
        "\n",
        "# # Download necessary NLTK models/data\n",
        "# nltk.download('punkt')\n",
        "# nltk.download('averaged_perceptron_tagger')\n",
        "# nltk.download('maxent_ne_chunker')\n",
        "# nltk.download('words')\n",
        "\n",
        "# # Example text\n",
        "# text = \"Barack Obama was born in Hawaii. He was elected president in 2008.\"\n",
        "\n",
        "# # Tokenize the text into words\n",
        "# tokens = word_tokenize(text)\n",
        "\n",
        "# # Perform POS tagging on the tokens\n",
        "# pos_tags = pos_tag(tokens)\n",
        "\n",
        "# # Perform Named Entity Recognition (NER) on the POS tagged tokens\n",
        "# named_entities = ne_chunk(pos_tags)\n",
        "\n",
        "# # Print the named entities\n",
        "# print(named_entities)\n",
        "\n",
        "# # For a more readable output, we can define a function to extract named entities\n",
        "# def extract_named_entities(ne_tree):\n",
        "#     entities = []\n",
        "#     for subtree in ne_tree:\n",
        "#         if isinstance(subtree, nltk.Tree):  # If subtree is a named entity\n",
        "#             entity_name = \" \".join([token for token, pos in subtree.leaves()])\n",
        "#             entity_type = subtree.label()\n",
        "#             entities.append((entity_name, entity_type))\n",
        "   # return entities\n",
        "\n",
        "# # Extract and print the named entities in a readable format\n",
        "# entities = extract_named_entities(named_entities)\n",
        "# for entity in entities:\n",
        "#     print(f\"Entity: {entity[0]}, Type: {entity[1]}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VgHggPN77Ema"
      },
      "source": [
        "**In this example:**\n",
        "\n",
        "POS tagging assigns labels like:\n",
        "- **JJ**: Adjective (e.g., exciting)\n",
        "- **NNP**: Proper Noun (e.g., Language, Processing)\n",
        "- **VBZ**: Verb, 3rd person singular (e.g., is)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6tF1fwIA7Ema"
      },
      "source": [
        "## 5. Stop Words\n",
        "\n",
        "Stop words are common words that appear frequently in natural language but carry little meaningful content on their own. In the context of Natural Language Processing (NLP), stop words are often removed during the preprocessing stage because they don't contribute much to the text's core meaning and can add noise to analyses.\n",
        "\n",
        "### Why Are Stop Words Removed?\n",
        "\n",
        "- **Frequency**: Words like \"is\", \"the\", \"in\", \"on\" are extremely common and appear in almost every text. They are considered redundant in most NLP tasks.\n",
        "- **Noise Reduction**: Since they don't contain significant meaning, removing stop words helps to focus on words that convey more useful information.\n",
        "- **Efficiency**: Reducing the size of the text by eliminating these words can make algorithms faster and more efficient, especially when dealing with large datasets.\n",
        "\n",
        "### Examples of Stop Words:\n",
        "- **Articles**: \"the\", \"a\", \"an\"\n",
        "- **Pronouns**: \"he\", \"she\", \"it\", \"they\"\n",
        "- **Prepositions**: \"in\", \"on\", \"at\", \"by\"\n",
        "- **Conjunctions**: \"and\", \"or\", \"but\"\n",
        "- **Auxiliary verbs**: \"is\", \"are\", \"was\", \"were\"\n",
        "- **Other**: \"to\", \"from\", \"of\", \"with\", \"about\"\n",
        "\n",
        "### Example Sentence:\n",
        "- **Original**: \"The dog is running in the park.\"\n",
        "- **After Removing Stop Words**: \"dog running park\"\n",
        "\n",
        "In this example, the words \"The\", \"is\", and \"in\" are removed as stop words, leaving behind only the more meaningful words.\n",
        "\n",
        "### Practical Use of Stop Words Removal:\n",
        "- **Text Classification**: In tasks like sentiment analysis, stop words are removed so that models focus on important words (like \"happy\", \"good\", \"love\") that contribute to sentiment.\n",
        "- **Search Engines**: Stop words are ignored in search queries to improve search speed and accuracy.\n",
        "- **Topic Modeling**: By removing stop words, algorithms can identify the key topics from text more efficiently.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'during', \"you'll\", 'its', 'with', \"you've\", 'yours', \"weren't\", 'further', 'y', 'more', 'they', 'd', 'no', 'where', 'hadn', 'so', 'what', \"isn't\", 'do', \"shan't\", 'when', 'mustn', 'own', 'up', 'will', 'such', 'myself', \"mightn't\", 'once', 'only', 'some', 'needn', 'of', \"that'll\", \"should've\", 'this', \"needn't\", 'doing', 'as', 'she', 'into', 'now', 'being', \"won't\", 'out', 'wouldn', 'same', 'all', 'shan', 'it', 'than', 'at', 'is', 'll', 'here', \"hadn't\", 'we', 'itself', 'through', 'and', 'on', 'off', \"don't\", 'me', 'you', 'hasn', \"doesn't\", 'or', 'whom', 'hers', 'were', \"you'd\", 'to', 'isn', 'ma', \"mustn't\", 'did', 'he', 'his', 'against', 'how', 'has', 'before', \"didn't\", \"aren't\", 'aren', 'too', 'haven', 'these', 'an', 'few', 'shouldn', 'be', 'weren', \"you're\", 'are', 'by', \"shouldn't\", 'won', 'down', 'i', 'ain', 'ours', 's', 'not', 'about', 'himself', 'below', 'their', 'while', 'but', 'above', 'ourselves', 'doesn', 'didn', 'which', \"haven't\", 'them', 'yourselves', \"it's\", 'been', 'each', 'am', 'should', \"couldn't\", 'because', 'other', 'in', 'was', \"wasn't\", 't', 'from', 'after', 'having', 'have', 'theirs', 'there', 'don', 'm', 'if', 'yourself', 'wasn', \"wouldn't\", 'themselves', 'for', 'again', 'o', 've', \"she's\", 'him', 'any', 'most', 'just', 'both', 'that', 'over', 'why', 'then', 'a', 'very', 'the', 'herself', 'those', 'between', 'couldn', 'nor', 'our', 'who', 'under', 'does', 'can', 'my', \"hasn't\", 'had', 'until', 'mightn', 're', 'your', 'her'}\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords\n",
        "text\n",
        "\n",
        "tokens_text = word_tokenize(text) # tokenize the text into words\n",
        "\n",
        "nltk.download(\"stopwords\");\n",
        "\n",
        "stop_words = set(stopwords.words('english'))\n",
        "print(stop_words)\n",
        "\n",
        "# # here filter out the stop words..\n",
        "# filtered_tokens = [word for word in tokens_text if word.lower() not in stop_words]\n",
        "# print(filtered_tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "F_fNhPHA7Ema",
        "outputId": "02b3ecff-07f1-4266-83e2-fa38be6470a3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Filtered Tokens: ['sample', 'sentence', 'contains', 'stop', 'words', '.']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "from nltk.corpus import stopwords;\n",
        "\n",
        "# Download necessary NLTK resources\n",
        "nltk.download('stopwords');\n",
        "\n",
        "# Sample text\n",
        "text = \"This is a sample sentence that contains stop words.\"\n",
        "\n",
        "# Tokenize the text into words\n",
        "tokens = word_tokenize(text);\n",
        "\n",
        "# Get the stop words for English\n",
        "stop_words = set(stopwords.words('english'));\n",
        "\n",
        "# Filter out stop words\n",
        "filtered_tokens = [word for word in tokens if word.lower() not in stop_words]\n",
        "\n",
        "# Print the filtered tokens\n",
        "print(\"Filtered Tokens:\", filtered_tokens)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {},
      "outputs": [],
      "source": [
        "import string\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5-96I6D77Emb"
      },
      "source": [
        "## 6. Vectorization\n",
        "\n",
        "After cleaning and tokenizing the text, the next step in processing textual data for machine learning tasks is to convert it into numerical form. This is where **vectorization** comes in. Textual data is inherently unstructured, so we need to represent it in a structured, numerical form that algorithms can understand. There are two common strategies for vectorization: **Count Vectorization** and **TF-IDF Vectorization**.\n",
        "\n",
        "---\n",
        "\n",
        "### **1. Count Vectorization (Bag of Words Model)**\n",
        "\n",
        "\n",
        "Count vectorization is a simple technique where the text is represented as a matrix of word counts. Each document is represented as a vector where each element corresponds to the frequency of a unique word from the document's vocabulary. This model doesn't consider the context or word order, just how often a word appears.\n",
        "\n",
        "- **For a single document**: We count the frequency of each word.\n",
        "- **For multiple documents**: We build a matrix, where each row corresponds to a document, and each column corresponds to a word in the entire corpus. The cell value at position `(i, j)` will be the frequency of the word `j` in document `i`.\n",
        "\n",
        "#### **Example**:\n",
        "\n",
        "Let's say we have two documents:\n",
        "\n",
        "- **Document 1**: \"Apple is a fruit\"\n",
        "- **Document 2**: \"Apple is red\"\n",
        "\n",
        "We extract the unique words from both documents: `['Apple', 'is', 'a', 'fruit', 'red']`.\n",
        "\n",
        "Now, let's create the count matrix:\n",
        "\n",
        "| Document   | Apple | is | a | fruit | red |\n",
        "|------------|-------|----|---|-------|-----|\n",
        "| Document 1 | 1     | 1  | 1 | 1     | 0   |\n",
        "| Document 2 | 1     | 1  | 0 | 0     | 1   |\n",
        "\n",
        "---\n",
        "\n",
        "### **2. TF-IDF (Term Frequency - Inverse Document Frequency)**\n",
        "\n",
        "TF-IDF is a more advanced vectorization strategy that considers not only the frequency of a word in a document (Term Frequency) but also how unique or rare the word is across the entire corpus (Inverse Document Frequency). It is useful for reducing the weight of common words that appear in many documents and emphasizing the importance of words that are more unique.\n",
        "\n",
        "The **TF-IDF** score is the product of two components:\n",
        "- **Term Frequency (TF)**: This measures how frequently a term appears in a document. The formula for TF is:\n",
        "\n",
        "$$\n",
        "\\text{TF}(t, d) = \\frac{\\text{Number of times term } t \\text{ appears in document } d}{\\text{Total number of terms in document } d}\n",
        "$$\n",
        "\n",
        "- **Inverse Document Frequency (IDF)**: This measures how important a word is across the entire corpus. It gives higher weight to words that appear in fewer documents, indicating that they contain more information. The formula for IDF is:\n",
        "\n",
        "$$\n",
        "\\text{IDF}(t, D) = \\log \\left( \\frac{|D|}{1 + \\text{DF}(t)} \\right)\n",
        "$$\n",
        "\n",
        "\n",
        "  Where `DF(t)` is the number of documents containing the term `t`, and `|D|` is the total number of documents in the corpus.\n",
        "\n",
        "The final **TF-IDF** value for a term in a document is:\n",
        "\n",
        "$$\n",
        "\\text{TF-IDF}(t, d, D) = \\text{TF}(t, d) \\times \\text{IDF}(t, D)\n",
        "$$\n",
        "\n",
        "\n",
        "#### **Example**:\n",
        "\n",
        "Let’s use the same documents as before:\n",
        "\n",
        "- **Document 1**: \"Apple is a fruit\"\n",
        "- **Document 2**: \"Apple is red\"\n",
        "\n",
        "First, we calculate **Term Frequency (TF)**:\n",
        "- **Document 1 (TF for 'Apple')**: 1/4 = 0.25\n",
        "- **Document 2 (TF for 'Apple')**: 1/3 = 0.33\n",
        "\n",
        "Now, calculate **Inverse Document Frequency (IDF)**:\n",
        "- The word \"Apple\" appears in both documents, so `DF('Apple') = 2`.\n",
        "- There are 2 documents in total, so `IDF('Apple') = log(2 / (1 + 2)) ≈ 0.176`.\n",
        "\n",
        "Now, calculate the **TF-IDF** for \"Apple\":\n",
        "- **TF-IDF('Apple', Document 1)** = 0.25 * 0.176 = 0.044\n",
        "- **TF-IDF('Apple', Document 2)** = 0.33 * 0.176 = 0.058\n",
        "\n",
        "\n",
        "### **Comparison and Use Cases**\n",
        "\n",
        "| Feature                | Count Vectorization                     | TF-IDF Vectorization                        |\n",
        "|------------------------|-----------------------------------------|--------------------------------------------|\n",
        "| **Purpose**            | Counts word frequency in each document  | Weighs word frequency by importance across documents |\n",
        "| **Use Case**           | Useful for text classification or tasks where word frequency is key | Better for document classification, information retrieval, and search |\n",
        "\n",
        "---\n",
        "\n",
        "### **Conclusion**\n",
        "\n",
        "Both **Count Vectorization** and **TF-IDF** are important methods for converting text into numerical form for machine learning. Count Vectorization is useful for simpler problems, while TF-IDF is better for distinguishing between more meaningful terms and common words. Depending on the task, one may be more applicable than the other. For example:\n",
        "- **Count Vectorization** is good for simple classification tasks (e.g., spam detection).\n",
        "- **TF-IDF** is better for tasks requiring more nuanced understanding, such as document similarity or retrieval.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['austen-emma.txt', 'austen-persuasion.txt', 'austen-sense.txt', 'bible-kjv.txt', 'blake-poems.txt', 'bryant-stories.txt', 'burgess-busterbrown.txt', 'carroll-alice.txt', 'chesterton-ball.txt', 'chesterton-brown.txt', 'chesterton-thursday.txt', 'edgeworth-parents.txt', 'melville-moby_dick.txt', 'milton-paradise.txt', 'shakespeare-caesar.txt', 'shakespeare-hamlet.txt', 'shakespeare-macbeth.txt', 'whitman-leaves.txt']\n",
            "[Emma by Jane Austen 1816]\n",
            "\n",
            "VOLUME I\n",
            "\n",
            "CHAPTER I\n",
            "\n",
            "\n",
            "Emma Woodhouse, handsome, clever, and rich, with a comfortable home\n",
            "and happy disposition, seemed to unite some of the best blessings\n",
            "of existence; and had lived nearly twenty-one years in the world\n",
            "with very little to distress or vex her.\n",
            "\n",
            "She was the youngest of the two daughters of a most affectionate,\n",
            "indulgent father; and had, in consequence of her sister's marriage,\n",
            "been mistress of his house from a very early period.  Her mother\n",
            "had died t\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package gutenberg to\n",
            "[nltk_data]     C:\\Users\\Administrator\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package gutenberg is already up-to-date!\n"
          ]
        }
      ],
      "source": [
        "# pip install nltk\n",
        "\n",
        "import nltk\n",
        "nltk.download('gutenberg')\n",
        "\n",
        "from nltk.corpus import gutenberg\n",
        "# List all files in the Gutenberg Corpus\n",
        "print(gutenberg.fileids())\n",
        "\n",
        "# Load text from a specific file (e.g., Jane Austen's \"Emma\")\n",
        "text = gutenberg.raw('austen-emma.txt')\n",
        "print(text[:500])  # Print the first 500 characters\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ftw2dkhJ7Emb"
      },
      "source": [
        "## 7. ReGex in NLP\n",
        "\n",
        "Regular Expressions (ReGex) are sequences of characters that form a search pattern. In Natural Language Processing (NLP), ReGex is used for identifying, matching, and manipulating text data. It allows us to efficiently search for specific patterns in text, such as dates, phone numbers, or even specific words and phrases. ReGex is essential in preprocessing text, performing tokenization, extracting useful data, and handling noisy text (such as special characters or unwanted spaces).\n",
        "\n",
        "### How ReGex is Used in NLP:\n",
        "- **Text Cleaning**: Removing unnecessary characters (punctuation, extra spaces, etc.).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import re\n",
        "\n",
        "# Text input\n",
        "text = \"mikawambua@124\"\n",
        "\n",
        "# Extract only alphabetic characters\n",
        "alphabets = re.findall(r'[a-zA-Z]', text)\n",
        "\n",
        "# Combine the extracted characters into a single string\n",
        "result = ''.join(alphabets)\n",
        "\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['m', 'i', 'k', 'a', 'w', 'a', 'm', 'b', 'u', 'a']\n"
          ]
        }
      ],
      "source": [
        "import re\n",
        "text = \"mikawambua@124\"\n",
        "\n",
        "# extract word\n",
        "word = re.findall(r'[a-zA-Z]', text)\n",
        "print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mikawambuabenson\n"
          ]
        }
      ],
      "source": [
        "# \n",
        "text = \"mikawambua @124 benson\"\n",
        "\n",
        "# extract word\n",
        "word = re.sub(r'[^a-zA-Z]',\"\", text)\n",
        "print(word)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Example text containing an email address\n",
        "text = \"You can reach me at mikawambua@domain.com for further information.\"\n",
        "\n",
        "# Regex pattern for extracting email addresses\n",
        "email_pattern = r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\\.[a-zA-Z]{2,}'\n",
        "\n",
        "# Find all matches for the email pattern\n",
        "emails = re.findall(email_pattern, text)\n",
        "\n",
        "# Print the extracted email addresses\n",
        "print(emails)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "teller\n",
            "teller\n"
          ]
        }
      ],
      "source": [
        "text = \" *****  teller\"\n",
        "text1 = text.lstrip(\" * \")\n",
        "print(text1)\n",
        "\n",
        "\n",
        "text2 = text1.lstrip(\" \")\n",
        "print(text2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "o9HZYATd7Emb",
        "outputId": "d34ed60f-52e9-4a53-d501-d9ab82fffc98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello This is a test sentence\n"
          ]
        }
      ],
      "source": [
        "\n",
        "import re\n",
        "text = \"Hello! This is a test sentence.\"\n",
        "cleaned_text = re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', '', text)).strip()\n",
        "print(cleaned_text)  # Output: 'Hello This is a test sentence'\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDYCTkSc7Emb"
      },
      "source": [
        "\n",
        "\n",
        "- **Tokenization**: Splitting text into meaningful components like words or sentences.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b0gds3jn7Emb",
        "outputId": "e5d2e123-bebf-4a29-c805-52f94ed2f36d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['Hello', 'How', 'are', 'you']\n"
          ]
        }
      ],
      "source": [
        "\n",
        "text = \"Hello! How are you?\"\n",
        "words = re.findall(r'\\b\\w+\\b', text)\n",
        "print(words)  # Output: ['Hello', 'How', 'are', 'you']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nx6Ip5ps7Emb"
      },
      "source": [
        "\n",
        "\n",
        "- **Information Extraction**: Identifying entities such as emails, phone numbers, and dates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "FlXcSM667Emb",
        "outputId": "b069688a-839e-49e8-d4b8-08d6dc8824f5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "['support@example.com', 'mika@gmail.com', 'mika@wambua.co.ke', 'john.doe@gmail.com']\n"
          ]
        }
      ],
      "source": [
        "# email having the domain part @, gma\n",
        "text = \"Contact us at support@example.com or mika@gmail.com  mikabenson@mail mika@wambua.co.ke john.doe@gmail.com\"\n",
        "emails = re.findall(r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Za-z]{2,}\\b', text)\n",
        "print(emails)  # Output: ['support@example.com']\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sCpEFH0p7Emc"
      },
      "source": [
        "\n",
        "- **Pattern Matching**: Searching for specific word patterns or structures.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3UJUQMuo7Emc",
        "outputId": "09573905-f552-476a-ae65-cd6b4eaa417e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Hello This is a test sentence\n"
          ]
        }
      ],
      "source": [
        "\n",
        "   import re\n",
        "   text = \"Hello! This is a test sentence.\"\n",
        "   cleaned_text = re.sub(r'\\s+', ' ', re.sub(r'[^\\w\\s]', '', text)).strip()\n",
        "   print(cleaned_text)  # Output: 'Hello This is a test sentence'\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "learn-env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
